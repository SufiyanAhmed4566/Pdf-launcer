# robots.txt for PDF Pilot - Optimized for Google Search Console & SEO
# https://pdfpilot.in/robots.txt
# Last updated: 2025-10-04

# ========================================
# Default Rules for All Search Engines
# ========================================
User-agent: *
Allow: /

# Explicitly allow important pages for indexing
Allow: /$
Allow: /merge-pdf$
Allow: /split-pdf$
Allow: /compress-pdf$
Allow: /convert-pdf$
Allow: /delete-pdf-pages$
Allow: /reorder-pdf-pages$

# Allow static resources (CSS, JS, Images)
Allow: /static/
Allow: /*.css$
Allow: /*.js$
Allow: /*.jpg$
Allow: /*.jpeg$
Allow: /*.png$
Allow: /*.gif$
Allow: /*.svg$
Allow: /*.webp$

# ========================================
# Disallow Backend & API
# ========================================
# Block API endpoints - no SEO value
Disallow: /api/
Disallow: /api/*

# Block upload directory - temporary files
Disallow: /uploads/
Disallow: /uploads/*

# Block admin/backend areas
Disallow: /admin/
Disallow: /admin/*
Disallow: /login
Disallow: /logout

# Block session/temp directories
Disallow: /tmp/
Disallow: /temp/
Disallow: /cache/

# Block query parameters that don't change content
Disallow: /*?session_id=
Disallow: /*?session=
Disallow: /*?ref=
Disallow: /*?utm_
Disallow: /*?sort=

# ========================================
# Googlebot Specific Rules (Priority)
# ========================================
User-agent: Googlebot
Allow: /
Allow: /static/
Allow: /*.css
Allow: /*.js
Disallow: /api/
Disallow: /uploads/
Crawl-delay: 0

# Allow Google to render JavaScript
User-agent: Googlebot-Image
Allow: /static/
Allow: /*.jpg
Allow: /*.jpeg
Allow: /*.png
Allow: /*.gif
Allow: /*.svg
Allow: /*.webp

# ========================================
# Other Major Search Engines
# ========================================
User-agent: Bingbot
Allow: /
Allow: /static/
Disallow: /api/
Disallow: /uploads/
Crawl-delay: 0

User-agent: Slurp
Allow: /
Allow: /static/
Disallow: /api/
Crawl-delay: 1

User-agent: DuckDuckBot
Allow: /
Allow: /static/
Disallow: /api/
Crawl-delay: 1

User-agent: Baiduspider
Allow: /
Disallow: /api/
Crawl-delay: 2

User-agent: YandexBot
Allow: /
Disallow: /api/
Crawl-delay: 2

# ========================================
# Social Media Crawlers (Allow for OG tags)
# ========================================
User-agent: facebookexternalhit
Allow: /

User-agent: Twitterbot
Allow: /

User-agent: LinkedInBot
Allow: /

User-agent: WhatsApp
Allow: /

# ========================================
# Block Aggressive/Unwanted Bots
# ========================================
# SEO audit tools (usually not needed for small sites)
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: Rogerbot
Disallow: /

User-agent: BLEXBot
Disallow: /

# Known scrapers/bad bots
User-agent: SiteSnagger
Disallow: /

User-agent: WebCopier
Disallow: /

User-agent: HTTrack
Disallow: /

User-agent: WebZIP
Disallow: /

User-agent: WebStripper
Disallow: /

User-agent: Offline Explorer
Disallow: /

User-agent: Teleport
Disallow: /

User-agent: TeleportPro
Disallow: /

User-agent: WebReaper
Disallow: /

User-agent: WebSauger
Disallow: /

# AI scrapers (block unless you want AI training on your content)
User-agent: CCBot
Disallow: /

User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /

User-agent: cohere-ai
Disallow: /

# ========================================
# Sitemap Location (CRITICAL for GSC)
# ========================================
Sitemap: https://pdfpilot.in/sitemap.xml

# ========================================
# Additional Guidelines
# ========================================
# Crawl-delay: Not set globally to allow fast crawling
# Host: pdfpilot.in (optional, for Yandex)